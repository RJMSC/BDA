import java.io.IOException;
//import java.util.*;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.IntWritable;
//import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
//import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
//import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.conf.Configuration;
//import MaximumTemp.MaxTemperatureMapper;
//import MaxTemp.MaxTemperatureReducer;
public class MaxTemp {
// Mapper
 /*MaxTemperatureMapper class is static
 * and extends Mapper abstract class
 * having four Hadoop generics type
 * LongWritable, Text, Text, Text.
 */
 public static class MaxTemperatureMapper extends
 Mapper<LongWritable, Text, Text, IntWritable> {
 // the data in our data set with
 // this value is inconsistent data
 //public static final int MISSING = 9999;
 public void map(LongWritable key, Text value, Context context)
 throws IOException, InterruptedException {
 String line=value.toString();
 String year=line.substring(15,19);
 int airtemp;
 if(line.charAt(87)=='+')
 {
 airtemp=Integer.parseInt(line.substring(88,92));
 }
 else
 airtemp=Integer.parseInt(line.substring(87,92));
 String q=line.substring(92,93);
 if(airtemp!=9999 && q.matches("[01459]"))
 {
 context.write(new Text(year), new IntWritable(airtemp));
 }
 }
 }
// Reducer
 /*MaxTemperatureReducer class is static
 and extends Reducer abstract class
 having four Hadoop generics type
 Text, Text, Text, Text.
 */
 public static class MaxTemperatureReducer extends
 Reducer<Text, IntWritable, Text, IntWritable> {
 /**
 * @method reduce
 * This method takes the input as key and
 * list of values pair from the mapper,
 * it does aggregation based on keys and
 * produces the final context.
 */
 public void reduce(Text key,Iterable<IntWritable> values, Context context)
 throws IOException, InterruptedException {
 int maxvalue= Integer.MIN_VALUE;
 for (IntWritable value : values) {
 maxvalue=Math.max(maxvalue, value.get());
 }
 context.write(key, new IntWritable(maxvalue));
 }
 }
 /**
 * @method main
 * This method is used for setting
 * all the configuration properties.
 * It acts as a driver for map-reduce
 * code.
 */
 public static void main(String[] args) throws Exception {
 // reads the default configuration of the
 // cluster from the configuration XML files
 Configuration conf = new Configuration();
 // Initializing the job with the
 // default configuration of the cluster
 // Job job = new Job(conf, "weather example");
 Job job = Job.getInstance(conf, "weather example");
 // Assigning the driver class name
 job.setJarByClass(MaxTemp.class);
 // Key type coming out of mapper
 // job.setMapOutputKeyClass(Text.class);
 // value type coming out of mapper
 // job.setMapOutputValueClass(Text.class);
 // Defining the mapper class name
 job.setMapperClass(MaxTemperatureMapper.class);
 // Defining the reducer class name
 job.setReducerClass(MaxTemperatureReducer.class);
 // Defining input Format class which is
 // responsible to parse the dataset
 // into a key value pair
 job.setInputFormatClass(TextInputFormat.class);
 // Defining output Format class which is
 // responsible to parse the dataset
 // into a key value pair
 job.setOutputFormatClass(TextOutputFormat.class);
 job.setOutputKeyClass(Text.class);
 job.setOutputValueClass(IntWritable.class);
 FileInputFormat.addInputPath(job, new Path(args[0]));
 FileOutputFormat.setOutputPath(job, new Path(args[1]));
 System.exit(job.waitForCompletion(true) ? 0 : 1);
 }
}
